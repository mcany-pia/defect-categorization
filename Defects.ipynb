{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "k8zX8ugi70_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KMhpcq_H9ADL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWG4uLsFsCOa"
      },
      "source": [
        "Initially used numpy for reading csv but it gave error at line 38 because there was a comma. So i guess it is not for reading text. therefore used pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O78Kybhgr0oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2957f934-4767-403c-8f8e-6093b1a63088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of                                                  summary       category  \\\n",
            "0      Incorrect data format received from the third-...     Data Error   \n",
            "1      System crashed due to unexpected null value in...     Data Error   \n",
            "2      Unexpected datatype in the 'userAge' field cau...     Data Error   \n",
            "3      The system could not process received XML data...     Data Error   \n",
            "4      Unable to parse the incoming JSON data due to ...     Data Error   \n",
            "...                                                  ...            ...   \n",
            "30395  The test script relied on outdated APIs,result...  Testing Error   \n",
            "30396  The test script did not handle session timeout...  Testing Error   \n",
            "30397  The test execution was affected by network con...  Testing Error   \n",
            "30398  The test script encountered issues with handli...  Testing Error   \n",
            "30399  The test execution failed due to an issue with...  Testing Error   \n",
            "\n",
            "                       super,,,,,  \n",
            "0      Data Management Error,,,,,  \n",
            "1      Data Management Error,,,,,  \n",
            "2      Data Management Error,,,,,  \n",
            "3      Data Management Error,,,,,  \n",
            "4      Data Management Error,,,,,  \n",
            "...                           ...  \n",
            "30395           Testing Error,,,,  \n",
            "30396           Testing Error,,,,  \n",
            "30397           Testing Error,,,,  \n",
            "30398           Testing Error,,,,  \n",
            "30399          Testing Error,,,,,  \n",
            "\n",
            "[30400 rows x 3 columns]>\n",
            "<bound method NDFrame.head of                                                  summary        category  \\\n",
            "30400  The test script did not properly handle browse...   Testing Error   \n",
            "30401  The test script encountered issues with handli...   Testing Error   \n",
            "30402  The test case did not handle unexpected respon...   Testing Error   \n",
            "30403  The test script did not properly handle browse...   Testing Error   \n",
            "30404  The test execution was affected by changes in ...   Testing Error   \n",
            "...                                                  ...             ...   \n",
            "37994  After a rollback,the application's caches or t...  Rollback Error   \n",
            "37995  A rollback error prevents the application from...  Rollback Error   \n",
            "37996  The application's rollback mechanism does not ...  Rollback Error   \n",
            "37997  After a rollback,the application's user interf...  Rollback Error   \n",
            "37998  A rollback error occurs when trying to cancel ...  Rollback Error   \n",
            "\n",
            "                  super,,,,,  \n",
            "30400      Testing Error,,,,  \n",
            "30401      Testing Error,,,,  \n",
            "30402      Testing Error,,,,  \n",
            "30403      Testing Error,,,,  \n",
            "30404      Testing Error,,,,  \n",
            "...                      ...  \n",
            "37994    Verfigback Error,,,  \n",
            "37995  Verfigback Error,,,,,  \n",
            "37996  Verfigback Error,,,,,  \n",
            "37997   Verfigback Error,,,,  \n",
            "37998  Verfigback Error,,,,,  \n",
            "\n",
            "[7599 rows x 3 columns]>\n"
          ]
        }
      ],
      "source": [
        "# train data\n",
        "tdf = pd.read_csv(\"CompleteDataset.csv\", sep=\"|\")\n",
        "# test data\n",
        "#vdf = pd.read_csv('defects.csv')\n",
        "cdf = vdf.sample(frac = 1, random_state=1).reset_index()\n",
        "#print(tdf.head)\n",
        "\n",
        "vdf = tdf.iloc[30400:]\n",
        "tdf = tdf.iloc[:30400]\n",
        "\n",
        "print(tdf.head)\n",
        "print(vdf.head)\n",
        "\n",
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfaCPT0NsQr2"
      },
      "source": [
        "Get number of samples and words per sample to compare. Then using https://developers.google.com/machine-learning/guides/text-classification/step-2-5 decide to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKmT7aQfsPsv",
        "outputId": "44454e15-d875-41fa-e816-a0583559130f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.276476471207495"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "rows = len(df)\n",
        "total_words = df['summary'].apply(lambda x: len(str(x).split())).sum()\n",
        "w_per_s = total_words / rows\n",
        "w_per_s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcUd7e6zNtGm"
      },
      "source": [
        "Since it is less than 1500 we are going with MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b38hVaoKNupk"
      },
      "outputs": [],
      "source": [
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        val_texts: list, validation text strings.\n",
        "\n",
        "    # Returns\n",
        "        x_train, x_val: vectorized training and validation texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
        "            'dtype': 'int32',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train).astype('float32')\n",
        "    x_val = selector.transform(x_val).astype('float32')\n",
        "    return x_train, x_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3J-N-tLN0E_"
      },
      "source": [
        "Berfore we fed model with data. We should tokenize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThszbVS3vc8l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "c26820aa-e8fd-4b9b-be7a-60c3f8ad92ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'super'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-07d4e79063fd>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define train_texts, train_labels, val_texts, and val_labels earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'super'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'super'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'super'"
          ]
        }
      ],
      "source": [
        "# Define train_texts, train_labels, val_texts, and val_labels earlier\n",
        "train_texts = tdf['summary']\n",
        "train_labels = tdf['super']\n",
        "val_texts = vdf['summary']\n",
        "val_labels = vdf['super']\n",
        "# Initialize a label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the training labels and transform both train and validation labels\n",
        "\n",
        "y_train_encoded = label_encoder.fit_transform(train_labels)\n",
        "y_val_encoded = label_encoder.transform(val_labels)\n",
        "unique_classes = label_encoder.classes_\n",
        "\n",
        "print(unique_classes)\n",
        "\n",
        "# Vectorize the texts using the function\n",
        "x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
        "\n",
        "# Initialize the MLP classifier\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=100, activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "mlp_clf.fit(x_train, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred_mlp = mlp_clf.predict(x_val)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_mlp = accuracy_score(y_val_encoded, y_val_pred_mlp, normalize=True)\n",
        "print(accuracy_mlp)\n",
        "cm = confusion_matrix(y_val_encoded, y_val_pred_mlp)\n",
        "fig = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_classes)\n",
        "fig.plot()\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_val_encoded, y_val_pred_mlp))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4aewcn9-A_UE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UWSvx_NE3wJDYrvBZgQXj37td19FmcFI",
      "authorship_tag": "ABX9TyNyovM7C3sGz0ZLKRpl39Ds"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}